{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\", verify = False)\n",
    "page_data=bs(page.content)\n",
    "header = page_data.find('h2', class_='mp-h2').text\n",
    "j=0\n",
    "for i in page_data.find_all('h2', class_='mp-h2'):\n",
    "    print(j+1, i.text)\n",
    "    j=j+1\n",
    "print(\"no of headers in main page of Wikipedia is \", j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSLError: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Main_Page (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_250=requests.get(\"https://www.imdb.com/chart/top/\", verify = False)\n",
    "IMDB_250_DATA=bs(IMDB_250.content)\n",
    "Indx=[]\n",
    "Mname=[]\n",
    "Myear=[]\n",
    "Mrating=[]\n",
    "\n",
    "for i in IMDB_250_DATA.find_all(\"td\", class_=\"titleColumn\"):\n",
    "    \n",
    "    Indx.append(i.text.split('\\n')[1].strip())\n",
    "    Mname.append(i.text.split('\\n')[2].strip())\n",
    "    Myear.append(i.text.split('\\n')[3][1:5])\n",
    "\n",
    "for k in IMDB_250_DATA.find_all('td', class_='ratingColumn imdbRating'):\n",
    "    Mrating.append(k.text.replace('\\n',''))\n",
    "\n",
    "test = pd.DataFrame({\"Rank\":Indx, \"Movie_Name\":Mname, \"Rating\":Mrating, \"ReleaseYear\":Myear})\n",
    "top100=test.loc[0:99,:]\n",
    "top100\n",
    "print(\"IMDb TOP 100 Movies list\",'\\n',top100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndMovie_Data=requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/\", verify=False)\n",
    "IndMovie250_Data=bs(IndMovie_Data.content)\n",
    "rank=[]\n",
    "movie=[]\n",
    "year=[]\n",
    "Rating=[]\n",
    "for i in IndMovie250_Data.find_all(\"td\", class_='titleColumn'):\n",
    "    rank.append(i.text.split('\\n')[1].strip().replace('.',''))\n",
    "    movie.append(i.text.split('\\n')[2].strip())\n",
    "    year.append(i.text.split('\\n')[3][1:5])\n",
    "\n",
    "for j in IndMovie250_Data.find_all('td', class_=\"ratingColumn imdbRating\"):\n",
    "    Rating.append(j.text.replace('\\n',''))\n",
    "IndTop_250=pd.DataFrame({\"Rank\":rank,\"Movie_Name\":movie,\"Rating\":Rating,\"Movie_Year\":year})\n",
    "print(\"IMDb top 100 Indian Movies\",'\\n',IndTop_250.loc[0:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "\n",
    "    i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "    ii) Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "\n",
    "    iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICC_URL=requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\", verify=False)\n",
    "ICCMODI_DATA=bs(ICC_URL.content)\n",
    "Rank=[]\n",
    "Cname=[]\n",
    "Nmatches=[]\n",
    "Points=[]\n",
    "Rating=[]\n",
    "first_team=ICCMODI_DATA.find('tr', class_=\"rankings-block__banner\")\n",
    "D=first_team.text.replace('\\n', ' ').strip().split('  ')\n",
    "Rank.append(D[0])\n",
    "Cname.append(D[1])\n",
    "Nmatches.append(D[2].split(' ')[0])\n",
    "Points.append(D[2].split(' ')[1])\n",
    "Rating.append(D[-1])\n",
    "\n",
    "for i in ICCMODI_DATA.find_all('tr', class_='table-body'):\n",
    "    d=i.text.replace('\\n', ' ').strip().split('  ')\n",
    "    Rank.append(d[0])\n",
    "    Cname.append(d[1])\n",
    "    Nmatches.append(d[2].split(' ')[0])\n",
    "    Points.append(d[2].split(' ')[1])\n",
    "    Rating.append(d[2].split(' ')[2])\n",
    "TOPODITEAMS=pd.DataFrame({\"Rank\":Rank,\"Country\":Cname,\"Matches\":Nmatches,\"Points\":Points,\"Rating\":Rating}).loc[:9]\n",
    "print(\"            ICC Top 10 ODI teams in men’s cricket\",'\\n',TOPODITEAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICC_BURL=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\", verify= False)\n",
    "ICCBODI_DATA=bs(ICC_BURL.content)\n",
    "pos=[]\n",
    "Nplayer=[]\n",
    "Team=[]\n",
    "Rating=[]\n",
    "batsman= ICCBODI_DATA.find('div', class_=\"rankings-block__top-player\")\n",
    "D=batsman.text.replace('\\n',' ').replace('     ','').split('  ')\n",
    "pos.append(D[0])\n",
    "Nplayer.append(D[1])\n",
    "Team.append(D[2])\n",
    "Rating.append(D[-1])\n",
    "for i in ICCBODI_DATA.find_all('tr', class_=\"table-body\"):\n",
    "    d=i.text.strip().replace('\\n',' ').replace('     ','').split('   ')\n",
    "    pos.append(d[0].split(' ', maxsplit=1)[0])\n",
    "    Nplayer.append(d[0].split(' ', maxsplit=1)[1])\n",
    "    Team.append(d[1].strip().split(' ')[0])\n",
    "    Rating.append(d[1].split(' ')[-1])\n",
    "players=pd.DataFrame({\"Rank\":pos,\"Player_Name\":Nplayer,\"Team\":Team,\"Rating\":Rating}).loc[:9]\n",
    "print(\"          Top 10 ODI Batsmen in men\",'\\n',players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_url=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling', verify=False)\n",
    "Bowler_DATA=bs(bowler_url.content)\n",
    "rank=[]\n",
    "name=[]\n",
    "team=[]\n",
    "rating=[]\n",
    "bowler1=Bowler_DATA.find('tr',class_=\"rankings-block__banner\")\n",
    "D=bowler1.text.replace('\\n',' ').strip().split('        ')[0:5]\n",
    "rank.append(D[0])\n",
    "name.append(D[1].split('    ')[0])\n",
    "team.append(D[1].split('    ')[1])\n",
    "rating.append(D[-1])\n",
    "for i in Bowler_DATA.find_all('tr',class_='table-body'):\n",
    "    t=i.text.replace('\\n',' ').strip().split('        ')[0:3]\n",
    "    rank.append(t[0])\n",
    "    name.append(t[1].strip().split('  ')[0])\n",
    "    team.append(t[1].strip().split('  ')[-2:-1])\n",
    "    rating.append(t[1].strip().split('  ')[-1])\n",
    "N=Bowler_DATA.find_all('tr',class_='table-body')\n",
    "\n",
    "BOWLER_RANKING=pd.DataFrame({\"Rank\":rank,\"Name\":name,\"Team\":team,\"Rating\":rating}).loc[:9]\n",
    "print(\"        ICC Top 10 ODI bowlers\",'\\n',BOWLER_RANKING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5.Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "        i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "        ii) Top 10 women’s ODI players along with the records of their team and rating.\n",
    "        iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WODI_URL=requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\", verify=False)\n",
    "WODI_DATA=bs(WODI_URL.content)\n",
    "rank=[]\n",
    "country=[]\n",
    "matches=[]\n",
    "points=[]\n",
    "rating=[]\n",
    "stw=WODI_DATA.find('tr', class_=\"rankings-block__banner\")\n",
    "a=stw.text.replace('\\n',' ').strip().split('          ')\n",
    "rank.append(a[0].split(\"  \")[0])\n",
    "country.append(a[0].split('  ')[1])\n",
    "matches.append(a[0].split(' ')[-2])\n",
    "points.append(a[0].split(' ')[-1])\n",
    "rating.append(a[-1])\n",
    "a=WODI_DATA.find_all('tr', class_=\"table-body\")\n",
    "for i in a:\n",
    "    rank.append(i.text.replace('\\n',' ').strip().split('  ')[0])\n",
    "    country.append(i.text.replace('\\n',' ').strip().split('  ')[1])\n",
    "    matches.append(i.text.replace('\\n',' ').strip().split('  ')[2].split()[0])\n",
    "    points.append(i.text.replace('\\n',' ').strip().split('  ')[2].split()[1])\n",
    "    rating.append(i.text.replace('\\n',' ').strip().split('  ')[2].split()[-1])\n",
    "W=pd.DataFrame({'Rank':rank,'Team':country,'Matches':matches,'Points':points,'Rating':rating})\n",
    "print(\"Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\",'\\n',W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 women’s ODI players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICC_BURL=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\", verify= False)\n",
    "ICCBODI_DATA=bs(ICC_BURL.content)\n",
    "pos=[]\n",
    "Nplayer=[]\n",
    "Team=[]\n",
    "Rating=[]\n",
    "batsman= ICCBODI_DATA.find('div', class_=\"rankings-block__top-player\")\n",
    "D=batsman.text.replace('\\n',' ').replace('     ','').split('  ')\n",
    "pos.append(D[0])\n",
    "Nplayer.append(D[1])\n",
    "Team.append(D[2])\n",
    "Rating.append(D[-1])\n",
    "for i in ICCBODI_DATA.find_all('tr', class_=\"table-body\"):\n",
    "    d=i.text.strip().replace('\\n',' ').replace('     ','').split('   ')\n",
    "    pos.append(d[0].split(' ', maxsplit=1)[0])\n",
    "    Nplayer.append(d[0].split(' ', maxsplit=1)[1].strip())\n",
    "    Team.append(d[1].strip().split(' ')[0])\n",
    "    Rating.append(d[1].split(' ')[-1])\n",
    "players=pd.DataFrame({\"Rank\":pos,\"Player_Name\":Nplayer,\"Team Name\":Team,\"Rating\":Rating}).loc[:9]\n",
    "print(\"          Top 10 ODI Batsmen in women\",'\\n',players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_url=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder', verify=False)\n",
    "Bowler_DATA=bs(bowler_url.content)\n",
    "rank=[]\n",
    "name=[]\n",
    "team=[]\n",
    "rating=[]\n",
    "bowler1=Bowler_DATA.find('tr',class_=\"rankings-block__banner\")\n",
    "D=bowler1.text.replace('\\n',' ').strip().split('        ')[0:5]\n",
    "rank.append(D[0])\n",
    "name.append(D[1].split('    ')[0])\n",
    "team.append(D[1].split('    ')[1])\n",
    "rating.append(D[-1])\n",
    "for i in Bowler_DATA.find_all('tr',class_='table-body'):\n",
    "    t=i.text.replace('\\n',' ').strip().split('        ')[0:3]\n",
    "    rank.append(t[0])\n",
    "    name.append(t[1].strip().split('  ')[0])\n",
    "    team.append(t[1].strip().split('  ')[-2:-1])\n",
    "    rating.append(t[1].strip().split('  ')[-1])\n",
    "N=Bowler_DATA.find_all('tr',class_='table-body')\n",
    "ALLROUND_RANKING=pd.DataFrame({\"Rank\":rank,\"Name\":name,\"Team\":team,\"Rating\":rating}).loc[:9]\n",
    "print(\"Top 10 women’s ODI all-rounder with the records of their team and rating.\",'\\n',ALLROUND_RANKING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6.Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue:I am getting API 503 error, so I tried it on flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=[]\n",
    "Price=[]\n",
    "Rating=[]\n",
    "Image=[]\n",
    "r=range(1,11)\n",
    "for u in r:\n",
    "    U='https://www.flipkart.com/search?q=mobiles&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&p%5B%5D=facets.price_range.from%3DMin&p%5B%5D=facets.price_range.to%3D20000&page='+str(u)\n",
    "    url=requests.get(U,verify=False)\n",
    "    data=bs(url.content)\n",
    "    mobile=data.find_all('div', class_='_4rR01T')\n",
    "    for i in mobile:\n",
    "        Name.append(i.text)\n",
    "    price=data.find_all('div',class_='_30jeq3 _1_WHN1')\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "    rating=data.find_all('div',class_='_3LWZlK')\n",
    "    for i in rating:\n",
    "        Rating.append(i.text)\n",
    "    image=data.find_all('img',class_='_396cs4 _3exPp9')\n",
    "    for i in image:\n",
    "        Image.append(i['src'])   \n",
    "        \n",
    "Data=pd.DataFrame({'Mobile Name':Name, 'Price':Price, 'Rating':Rating,'Image URL':Image})\n",
    "print(\"Mobiles under Rs:20000 from Flipkart\",'\\n',Data.sort_values(by=\"Price\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape house details from mentioned url. It should include house title, location, area, emi and price\n",
    "https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44N DUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8 iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House=[]\n",
    "Location=[]\n",
    "Area=[]\n",
    "Emi=[]\n",
    "Cost=[]\n",
    "lt=['RK1','BHK1','BHK2','BHK3','BHK4','BHK4PLUS']\n",
    "for j in lt:\n",
    "    \n",
    "    url=requests.get('https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type='+j+'&searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0', verify=False)\n",
    "    data=bs(url.content)\n",
    "    house=data.find_all('h2', class_='heading-6 font-semi-bold nb__1AShY')\n",
    "    for i in house:\n",
    "        House.append(i.text)\n",
    "    location=data.find_all('div', class_='nb__35Ol7')\n",
    "    for i in location:\n",
    "        Location.append(i.text)\n",
    "    area=data.find_all('div', class_='nb__3oNyC')\n",
    "    for i in area:\n",
    "        Area.append(i.text)\n",
    "    emi=data.find_all('div',id='roomType')\n",
    "    for i in emi:\n",
    "        Emi.append(i.text)\n",
    "    cost=data.find_all('div', id=\"minDeposit\")\n",
    "    for i in cost:\n",
    "        Cost.append(i.text.replace('₹',' ').strip().split(' ')[0]+i.text.replace('₹',' ').strip().split(' ')[1])\n",
    "Details=pd.DataFrame({\"House Details\":House,'Location':Location,'Area':Area, 'EMI Details':Emi, 'Cost':Cost})\n",
    "Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Write a python program to scrape mentioned details from ‘https://www.dineout.co.in/delhi-restaurants/buffet-special’ :\n",
    "        i) Restaurant name\n",
    "        ii) Cuisine\n",
    "        iii) Location\n",
    "        iv) Ratings\n",
    "        v) Image url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL=[]\n",
    "R=[]\n",
    "C=[]\n",
    "L=[]\n",
    "RA=[]\n",
    "I=[]\n",
    "url=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special', verify=False)\n",
    "ur=bs(url.content)\n",
    "re=ur.find_all('a', class_=\"restnt-name ellipsis\")\n",
    "for i in re:\n",
    "    RL.append(i['href'])\n",
    "RL\n",
    "for i in RL:\n",
    "    u='https://www.dineout.co.in/'+i\n",
    "    ur1=requests.get(u, verify=False)\n",
    "    re1=bs(ur1.content)\n",
    "    r=re1.find('div',class_='restnt-details_info')\n",
    "    R.append(r.text.split('₹')[0])\n",
    "    c=re1.find('div',class_='restnt-cost')\n",
    "    C.append(c.text.split('|')[-1])\n",
    "    l=re1.find('div',class_='restnt-name')\n",
    "    L.append(l.text.replace('|','').replace(\"Get Direction\",'').strip())\n",
    "    r=re1.find('section',class_='rdp-section restnt-details d-flex')\n",
    "    RA.append(r.text[-13:])\n",
    "    i=re1.find('img',class_='rdp-banner_img')\n",
    "    I.append(i['src'])\n",
    "\n",
    "final=pd.DataFrame({\"Restaurant name\":R,'Cuisine':C,'Location':L,'Rating':RA,\"Image url\":I})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       9. Write a python program to scrape weather details for last 24 hours from ‘https://en.tutiempo.net/delhi.html?data=last-24- hours’ :\n",
    "            i) Hour\n",
    "            ii) Temperature\n",
    "            iii) Wind\n",
    "            iv) Weather condition\n",
    "            v) Humidity\n",
    "            vi) Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_url=requests.get('https://en.tutiempo.net/delhi.html?data=last-24-hours', verify=False)\n",
    "weather_data=bs(weather_url.content)\n",
    "time=weather_data.find('div', class_='last24 thh')\n",
    "w=time.text.replace('.Pressure','.Presssure, ').replace('hPa', 'hPa, ').split(',')[:24]\n",
    "hours=[]\n",
    "We=[]\n",
    "Tem=[]\n",
    "Wi=[]\n",
    "Hum=[]\n",
    "Pre=[]\n",
    "for i in w:\n",
    "    hours.append(i[:6])\n",
    "hours\n",
    "len(hours)\n",
    "we=weather_data.find_all('span',class_='thhip ico i0530 u303')\n",
    "for i in we:\n",
    "    We.append(i.text)\n",
    "tem=weather_data.find_all('td',class_='t Temp')\n",
    "for i in tem:\n",
    "    Tem.append(i.text)\n",
    "len(Tem[:24])\n",
    "wi=weather_data.find_all('td',class_='wind')\n",
    "for i in wi:\n",
    "    Wi.append(i.text)\n",
    "hum=weather_data.find_all('td',class_='hr')\n",
    "for i in hum:\n",
    "    Hum.append(i.text)\n",
    "pre=weather_data.find_all('td',class_='prob')\n",
    "for i in pre:\n",
    "    Pre.append(i.text)\n",
    "H=hours\n",
    "print(\"1.Hour\",'\\n',hours,'\\n',\"2.Temperature\",'\\n',Tem,'\\n',\"3.Wind\",'\\n',Wi,'\\n',\"4.Weather Condition\",'\\n',We, '\\n',\"5.Humidity\",'\\n',Hum,'\\n',\"6.Pressure\",'\\n',Pre)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Write a python program to scrape monument name, monument description, image url about top 10 monuments from 'https://www.puredestinations.co.uk/top-10-famous-monuments-to-visit-in-india/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://www.puredestinations.co.uk/top-10-famous-monuments-to-visit-in-india/', verify=False)\n",
    "mo_data=bs(url.content)\n",
    "I=[]\n",
    "a=mo_data.find_all('p')\n",
    "for i in a:\n",
    "    I.append(i)\n",
    "i=I[4:-8].copy()\n",
    "name=[]\n",
    "des=[]\n",
    "img=[]\n",
    "n=0\n",
    "k=0\n",
    "l=1\n",
    "m=2\n",
    "while n<10:\n",
    "    name.append(i[k].text)\n",
    "    des.append(i[l].text)\n",
    "    img.append(i[m])\n",
    "    n=n+1\n",
    "    k=k+3\n",
    "    l=l+3\n",
    "    m=m+3\n",
    "print(\"TOP 10 Monuments Names\",'\\n',name, '\\n',\"TOP 10 Monuments Description\",'\\n',des,'\\n',\"TOP 10 Monuments Image URL's\",'\\n',img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
